{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy\n",
    "'''\n",
    "Below is the needed functionality for the track reconstruction evaluation\n",
    "'''\n",
    "\n",
    "def read_track_txt(track_dir):\n",
    "    raw_tracks = []\n",
    "    with open(track_dir, 'r') as file:\n",
    "        for track in file:\n",
    "            track = list(map(int, track.split()))\n",
    "            raw_tracks.append(track)\n",
    "\n",
    "    return raw_tracks\n",
    "\n",
    "def get_tracks(track_dir, min_num_hits):\n",
    "    '''\n",
    "    Input: List of track candidates\n",
    "    Output: List of tracks with at least min_num_hits hits\n",
    "    '''\n",
    "    raw_tracks = read_track_txt(track_dir)\n",
    "    track_candidates = [track for track in raw_tracks if len(track) >= min_num_hits]\n",
    "\n",
    "    return track_candidates\n",
    "\n",
    "def get_hit_particle_ids(hit_list, map):\n",
    "    '''\n",
    "    Input: List of hits\n",
    "    Output: List of particle ids for each hit\n",
    "    '''\n",
    "    hit_particle_ids = []\n",
    "\n",
    "    for hit in hit_list:\n",
    "        hit_particle_ids.append(map[hit]**2)\n",
    "\n",
    "    return hit_particle_ids\n",
    "\n",
    "def create_hit_particle_dict(particles_dir, truth_dir):\n",
    "    '''\n",
    "    Input: Directories to the particles and truth csv files\n",
    "    Output: Dictionary with hit_id as keys and particle_type as values\n",
    "    '''\n",
    "    particles_df = pd.read_csv(particles_dir)\n",
    "    truth_df = pd.read_csv(truth_dir)\n",
    "\n",
    "    # Merge the dataframes on particle_id\n",
    "    merged_df = pd.merge(truth_df[['hit_id', 'particle_id']], particles_df[['particle_id', 'particle_type']], on='particle_id')\n",
    "\n",
    "    # Create the dictionary with hit_id as keys and particle_type as values\n",
    "    hit_particle_dict = dict(zip(merged_df['hit_id'], merged_df['particle_type']))\n",
    "\n",
    "    return hit_particle_dict\n",
    "\n",
    "def clopper_pearson(passed: float, total: float, level: float = 0.68):\n",
    "    \"\"\"\n",
    "    Estimate the confidence interval for a sampled binomial random variable with Clopper-Pearson.\n",
    "    `passed` = number of successes; `total` = number trials; `level` = the confidence level.\n",
    "    The function returns a `(low, high)` pair of numbers indicating the lower and upper error bars.\n",
    "    \"\"\"\n",
    "    alpha = (1 - level) / 2\n",
    "    lo = scipy.stats.beta.ppf(alpha, passed, total - passed + 1) if passed > 0 else 0.0\n",
    "    hi = (\n",
    "        scipy.stats.beta.ppf(1 - alpha, passed + 1, total - passed)\n",
    "        if passed < total\n",
    "        else 1.0\n",
    "    )\n",
    "    average = passed / total if total != 0 else 0.0\n",
    "    return (average - lo, hi - average)\n",
    "\n",
    "def count_true_fake_tracks(dir, feature_store, min_track_length=4, samplesize=100000):\n",
    "    '''\n",
    "    This is the centerpiece for the track reconstruction evaluation!\n",
    "\n",
    "    Input:  Directory to the track txt files, feature store\n",
    "            min. track length default: 4, samplesize default: all events in dir\n",
    "    Output: Fake/reconstruction ratio with clopper pearson errors, raw numbers for fake/true/total tracks\n",
    "    '''\n",
    "\n",
    "    #Set counters\n",
    "    num_true = 0\n",
    "    num_fake = 0\n",
    "    num_evts = 0\n",
    "    num_of_tracks = []\n",
    "    err_reco = np.array([0,0])\n",
    "    err_fake = np.array([0,0])\n",
    "\n",
    "    fake_ratio = 0\n",
    "    reco_ratio = 0\n",
    "    \n",
    "    for set in ['testset']: #initially did this for train and valset as well but this is only allowed if the data has not been used in training\n",
    "        tracks_dir = dir + f'/{set}_tracks' #directory within testset, containing the track txt files\n",
    "        num_evts += len(os.listdir(tracks_dir)) #count the number of events\n",
    "        for track_txt in os.listdir(tracks_dir): #look at single track txt file -> can contain multiple track candidates\n",
    "            if num_evts > samplesize: #break if samplesize is reached\n",
    "                break\n",
    "\n",
    "            track_path = os.path.join(tracks_dir, track_txt) #compose path to track txt file\n",
    "\n",
    "            track_candidates = get_tracks(track_path, min_track_length) #filter track candidates with at least min_track_length hits\n",
    "            num_of_tracks.append(len(track_candidates)) #count number of track candidates\n",
    "\n",
    "            if len(track_candidates) == 0: #if there are no track candidates, continue\n",
    "                continue\n",
    "            \n",
    "            #read corresponding truth and particle csv files\n",
    "            truth_path = '/mnt/data1/karres/cosmics_test'+f'/{feature_store}'+f'/{set}'f'/{track_txt[:-4]}-truth.csv'\n",
    "            particle_path = '/mnt/data1/karres/cosmics_test'+f'/{feature_store}'+f'/{set}'f'/{track_txt[:-4]}-particles.csv'\n",
    "\n",
    "            #create particle id map for this event as explained above\n",
    "            particle_id_map = create_hit_particle_dict(particle_path, truth_path)\n",
    "            true_per_event = 0\n",
    "\n",
    "            for track in track_candidates: #loop over all correct track candidates\n",
    "                hit_particle_ids = get_hit_particle_ids(track, particle_id_map) #get particle ids for each hit in the track candidate\n",
    "                if all(x == 13**2 for x in hit_particle_ids):\n",
    "                    true_per_event += 1 #count track as true if all hits are muons (+-13) -> considers muons and anti muons as the same particle though -> should be changed perhaps\n",
    "\n",
    "            if true_per_event >= 1:\n",
    "                num_true += 1 #essentially count true triggers\n",
    "            else:\n",
    "                num_fake += 1 #count fake triggers\n",
    "\n",
    "    reco_ratio = num_true/num_evts #really only makes sense for cosmics only and cosmics w/ michel\n",
    "    fake_ratio = num_fake/num_evts\n",
    "    \n",
    "    err_reco = np.array(clopper_pearson(num_true, num_evts))\n",
    "    err_fake = np.array(clopper_pearson(num_fake, num_evts))\n",
    "    \n",
    "    return fake_ratio, reco_ratio, num_of_tracks, num_true, num_fake, err_reco[0], err_reco[1], err_fake[0], err_fake[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "stage_dir = '/mnt/data1/karres/cosmics_test/thesis_results/michel_only/' #containing folders to different gnn models\n",
    "\n",
    "#gnns = ['c_only', 'cm', 'cm_c', 'cm_m','cm_m_c'] #gnns for cosmics only -> cm_m did not work, therefore trained another gnn on cosmics only as well\n",
    "gnns = ['cm', 'cm_c', 'cm_m','cm_m_c'] #gnns for all other cases\n",
    "\n",
    "for gnn in gnns:\n",
    "    fake_ratios_3 = []\n",
    "    reco_ratios_3 = []\n",
    "    num_of_tracks_3 = []\n",
    "    num_true_3 = []\n",
    "    num_fake_3 = []\n",
    "    err_reco_3_down = []\n",
    "    err_reco_3_up = []\n",
    "    err_fake_3_down = []\n",
    "    err_fake_3_up = []\n",
    "\n",
    "    fake_ratios_4 = []\n",
    "    reco_ratios_4 = []\n",
    "    num_of_tracks_4 = []\n",
    "    num_true_4 = []\n",
    "    num_fake_4 = []\n",
    "    err_reco_4_down = []\n",
    "    err_reco_4_up = []\n",
    "    err_fake_4_down = []\n",
    "    err_fake_4_up = []\n",
    "\n",
    "    dir = stage_dir+gnn+'_gnn/connected_components/' #compose path to connected components folder of specific gnn infered on specific data\n",
    "\n",
    "    feature_store = 'feature_store_michel' #name of the feature store the gnn was infered on (fully connected graphs)\n",
    "\n",
    "    #connected components folder contains subfolders called 50, 60,...,90, 925, 950,...; meaning 0.5, 0.6, 0.9, 0.925, 0.950,...\n",
    "    score_cuts = [int(cut)/100 if int(cut)<100 else int(cut)/1000 for cut in sorted(os.listdir(dir)) if os.path.isdir(os.path.join(dir, cut))] #get list of score cuts found in that folder\n",
    "    size = 100000 #sample size\n",
    "\n",
    "    dir_list = [cut for cut in sorted(os.listdir(dir)) if os.path.isdir(os.path.join(dir, cut))] #get all subfolders in connected components folder\n",
    "\n",
    "    for cut in dir_list: #get track recon metrics for min track length 3 for every score cut individually\n",
    "        new_fake, new_reco, new_num, new_num_true, new_num_fake, new_err_reco_down, new_err_reco_up, new_err_fake_down, new_err_fake_up = count_true_fake_tracks(dir+cut, feature_store, 3, size)\n",
    "        fake_ratios_3.append(new_fake)\n",
    "        reco_ratios_3.append(new_reco)\n",
    "        num_of_tracks_3.append(new_num)\n",
    "        num_true_3.append(new_num_true)\n",
    "        num_fake_3.append(new_num_fake)\n",
    "        err_reco_3_down.append(new_err_reco_down)\n",
    "        err_reco_3_up.append(new_err_reco_up)\n",
    "        err_fake_3_down.append(new_err_fake_down)\n",
    "        err_fake_3_up.append(new_err_fake_up)\n",
    "\n",
    "    for cut in dir_list: #same for track length 4\n",
    "        new_fake, new_reco, new_num, new_num_true, new_num_fake, new_err_reco_down, new_err_reco_up, new_err_fake_down, new_err_fake_up = count_true_fake_tracks(dir+cut, feature_store, 4, size)\n",
    "        fake_ratios_4.append(new_fake)\n",
    "        reco_ratios_4.append(new_reco)\n",
    "        num_of_tracks_4.append(new_num)\n",
    "        num_true_4.append(new_num_true)\n",
    "        num_fake_4.append(new_num_fake)\n",
    "        err_reco_4_down.append(new_err_reco_down)\n",
    "        err_reco_4_up.append(new_err_reco_up)\n",
    "        err_fake_4_down.append(new_err_fake_down)\n",
    "        err_fake_4_up.append(new_err_fake_up)\n",
    "\n",
    "    #put all data in a dictionary and save it as a csv file\n",
    "    data_to_save = {'fr_3':fake_ratios_3, 'rr_3': reco_ratios_3, 'num_true_3': num_true_3, 'num_fake_3': num_fake_3, 'err_reco_3_down': err_reco_3_down, 'err_reco_3_up': err_reco_3_up, 'err_fake_3_down': err_fake_3_down, 'err_fake_3_up': err_fake_3_up,\n",
    "                'fr_4': fake_ratios_4, 'rr_4': reco_ratios_4, 'num_true_4': num_true_4, 'num_fake_4': num_fake_4, 'err_reco_4_down': err_reco_4_down, 'err_reco_4_up': err_reco_4_up, 'err_fake_4_down': err_fake_4_down, 'err_fake_4_up': err_fake_4_up}\n",
    "\n",
    "    (pd.DataFrame.from_dict(data=data_to_save, orient='index').to_csv(dir+'fake_rate_data_testset_only.csv', header=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "plot_dir = '/home/mue/karres/git/Mu3eCosmicGNN/examples/Cosmic_GNN/thesis_collection/' #dir to save plots\n",
    "\n",
    "cosmics_dir = '/mnt/data1/karres/cosmics_test/thesis_results/cosmics_only/' #cosmics only dir (contains \"fake_rate_data_testset_only.csv\" inside of connected components folder)\n",
    "michel_dir = '/mnt/data1/karres/cosmics_test/thesis_results/michel_only/'\n",
    "cosmic_michel_dir = '/mnt/data1/karres/cosmics_test/thesis_results/cosmics_with_michel/'\n",
    "\n",
    "score_cuts = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.925, 0.95, 0.975, 0.98, 0.985, 0.99, 0.995, 0.999]\n",
    "\n",
    "sample = 'Michel only' #appears in figure title\n",
    "filename = 'michel_only_large' #file name\n",
    "directory = michel_dir  #directory to the scored graphs\n",
    "\n",
    "gnns = ['cm_m_c_gnn','cm_m_gnn', 'cm_c_gnn', 'cm_gnn'] #folder names of the gnns within the chosen directory\n",
    "gnn_names = ['Cosmics w/ Michel + Michel + Cosmics', 'Cosmics w/ Michel + Michel', 'Cosmics w/ Michel + Cosmics', 'Cosmics w/ Michel'] #names of the gnns for the legend\n",
    "gnn_colours =  ['blue','orange', 'green', 'red']\n",
    "gnn_markers = ['o', 's', 'v', 'D']\n",
    "\n",
    "######################################################################################################\n",
    "#Fake vs Score\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax1.grid(True)\n",
    "ax2.grid(True)\n",
    "\n",
    "ax1.set_title('min. track length = 3')\n",
    "ax1.set_xlabel('1-Score cut')\n",
    "ax1.set_ylabel('Fake ratio') #This is the acceptance if plotting michel only\n",
    "\n",
    "ax2.set_title('min. track length = 4')\n",
    "ax2.set_xlabel('1-Score cut')\n",
    "ax2.set_ylabel('Fake ratio')\n",
    "\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_yscale('log')\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_yscale('log')\n",
    "#might need to set some limits for the axes for coherence\n",
    "\n",
    "for index, (gnn, gnn_name, colour, m) in enumerate(zip(gnns, gnn_names, gnn_colours, gnn_markers)): #read in eval data for every gnn individually\n",
    "    if index == 1 or index == 3: #skip gnns if not needed in plot\n",
    "        continue\n",
    "\n",
    "    path = directory + gnn + '/connected_components/fake_rate_data_testset_only.csv' \n",
    "    data = pd.read_csv(path, header=None, index_col=0)\n",
    "    data = data.apply(lambda row: row.tolist(), axis=1).to_dict() #convert to dictionary\n",
    "    \n",
    "    fr_3_err = np.array([data['err_fake_3_down'], data['err_fake_3_up']]) #fake ratio errors\n",
    "    fr_4_err = np.array([data['err_fake_4_down'], data['err_fake_4_up']])\n",
    "\n",
    "    ax1.errorbar(1-np.array(score_cuts), data['fr_3'], yerr = fr_3_err, marker=m, \n",
    "                 label=gnn_name, color=colour, capsize=5, linestyle='None', markersize=5)\n",
    "    ax2.errorbar(1-np.array(score_cuts), data['fr_4'], yerr = fr_4_err, marker=m, \n",
    "                 label=gnn_name, color=colour, capsize=5, linestyle='None', markersize=5)\n",
    "\n",
    "line, label = ax1.get_legend_handles_labels()\n",
    "fig.legend(line, label, loc='upper center', ncol=2, bbox_to_anchor=(0.5, 0.94)) #place legend on top of the plot\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.85])\n",
    "fig.suptitle(f\"Fake ratio at multiple score cuts, inferred on {sample}\", fontsize=15)\n",
    "fig.savefig(plot_dir + f\"fake_vs_score_{filename}.pdf\", format='pdf')\n",
    "plt.show()\n",
    "\n",
    "######################################################################################################\n",
    "#Reco vs Score\n",
    "#same spiel for reconstruction ratio vs score cut and then reconstruction ratio vs fake ratio afterwards\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax1.grid(True)\n",
    "ax2.grid(True)\n",
    "\n",
    "ax1.set_title('min. track length = 3')\n",
    "ax1.set_xlabel('1-Score cut')\n",
    "ax1.set_ylabel(r'$\\epsilon_\\text{recon}$')\n",
    "\n",
    "ax2.set_title('min. track length = 4')\n",
    "ax2.set_xlabel('1-Score cut')\n",
    "ax2.set_ylabel(r'$\\epsilon_\\text{recon}$')\n",
    "\n",
    "ax1.set_xscale('log')\n",
    "ax2.set_xscale('log')\n",
    "ax1.set_yscale('log')\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "for index, (gnn, gnn_name, colour, m) in enumerate(zip(gnns, gnn_names, gnn_colours, gnn_markers)):\n",
    "    if index == 10 or index == 30:\n",
    "        continue\n",
    "\n",
    "    path = directory + gnn + '/connected_components/fake_rate_data_testset_only.csv'\n",
    "    data = pd.read_csv(path, header=None, index_col=0)\n",
    "    data = data.apply(lambda row: row.tolist(), axis=1).to_dict()\n",
    "\n",
    "    rr_3_err = np.array([data['err_reco_3_down'], data['err_reco_3_up']])\n",
    "    rr_4_err = np.array([data['err_reco_4_down'], data['err_reco_4_up']])\n",
    "\n",
    "    ax1.errorbar(1-np.array(score_cuts), data['rr_3'], yerr = rr_3_err, marker=m, \n",
    "                 label=gnn_name, color=colour, capsize=5, linestyle='None', markersize=5)\n",
    "    ax2.errorbar(1-np.array(score_cuts), data['rr_4'], yerr = rr_4_err, marker=m, \n",
    "                 label=gnn_name, color=colour, capsize=5, linestyle='None', markersize=5)\n",
    "\n",
    "    #ax1.plot(score_cuts[6:], data['rr_3'][6:], marker=marker, label=gnn_name, color=colour)\n",
    "    #ax2.plot(score_cuts[6:], data['rr_4'][6:], marker=marker, label=gnn_name, color=colour)  \n",
    "\n",
    "line, label = ax1.get_legend_handles_labels()\n",
    "fig.legend(line, label, loc='upper center', ncol=2, bbox_to_anchor=(0.5, 0.94))\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.85])\n",
    "fig.suptitle(r'$\\epsilon_\\text{recon}$'+ f\" at multiple score cuts, inferred on {sample}\", fontsize=15)\n",
    "fig.savefig(plot_dir + f\"reco_vs_score_{filename}.pdf\", format='pdf')\n",
    "plt.show()\n",
    "\n",
    "###################################################################################################\n",
    "#Reco vs Fake\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax1.grid(True)\n",
    "ax2.grid(True)\n",
    "\n",
    "ax1.set_title('min. track length = 3')\n",
    "ax1.set_xlabel('Fake ratio')\n",
    "ax1.set_ylabel(r'$\\epsilon_\\text{recon}$')\n",
    "ax1.set_xscale('log')\n",
    "\n",
    "ax2.set_title('min. track length = 4')\n",
    "ax2.set_xlabel('Fake ratio')\n",
    "ax2.set_ylabel(r'$\\epsilon_{recon}$')\n",
    "ax2.set_xscale('log')\n",
    "\n",
    "for index, (gnn, gnn_name, colour, m) in enumerate(zip(gnns, gnn_names, gnn_colours, gnn_markers)):\n",
    "    if index == 10 or index == 30:\n",
    "        continue\n",
    "\n",
    "    path = directory + gnn + '/connected_components/fake_rate_data_testset_only.csv'\n",
    "    data = pd.read_csv(path, header=None, index_col=0)\n",
    "    data = data.apply(lambda row: row.tolist(), axis=1).to_dict()\n",
    "\n",
    "    ax1.plot(data['fr_3'], np.array(data['rr_3']), marker=m, label=gnn_name, color=colour, markersize=5, linestyle='None')\n",
    "    ax2.plot(data['fr_4'], np.array(data['rr_4']), marker=m, label=gnn_name, color=colour, markersize=5, linestyle='None')  \n",
    "\n",
    "line, label = ax1.get_legend_handles_labels()\n",
    "fig.legend(line, label, loc='upper center', ncol=2, bbox_to_anchor=(0.5, 0.94))\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.85])\n",
    "fig.suptitle(r\"$\\epsilon_\\text{recon}$\" + f\" as a function of the fake ratio, inferred on {sample}\", fontsize=15)\n",
    "fig.savefig(plot_dir + f\"reco_vs_fake_{filename}.pdf\", format='pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cosmicgnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
